%
% Zusammenfassung Image Analysis & Computer Vision D-ITET
% ===========================================================================
% Author:			Marco Dober
% Version:			0.1
% Last changed: 	19.09.2019	
% ---------------------------------------------------------------------------

\documentclass[a4paper, fontsize=8pt, landscape, DIV=1]{scrartcl}
\usepackage{lastpage}
\usepackage{hyperref, makecell}
\usepackage[graphicx]{realboxes}
% Include general settings and customized commands
\input{settings/general}
\input{settings/commands}
%change page style for header
\pagestyle{fancy}
\footskip 20pt
\rhead{Marco Dober}
\lhead{Image Analysis \& Computer Vision}
\chead{\thepage}
\cfoot{}
\headheight 17pt \headsep 10pt
\title{Image Analysis \& Computer Vision}
\author{Marco Dober}
\date{\today}


\begin{document}
	\setcounter{secnumdepth}{3} %no enumeration of sections
	\begin{multicols*}{4}
		%
		\section*{Disclaimer}
		This summary is part of the lecture ``ETH Image Analysis \& Computer Vision'' by Prof. Van Gool, Prof. Konukoglu and Prof. Goksel  (HS19). It is based on the lecture slides and script. \\[6pt]
		Please report errors to \href{mailto:doberm@student.ethz.ch}{doberm@student.ethz.ch} such that others can benefit as well.\\[6pt]	
		The upstream repository can be found at \href{https://github.com/mrrebod/Summaries}{https://github.com/mrrebod/Summaries}
		\vfill\null
		\pagebreak
		
		\maketitle 
		\thispagestyle{fancy}
		
		\section{Introduction}
		Vision is important:
		\begin{itemize}[noitemsep, label={$\blacktriangleright$}]
			\item Half our brain is devoted to it
			\item Developed many times douring evolution
			\item It is non-contact
			\item It can be implemenmted with high-resolution
			\item Works with ambient EM-waves
			\item yields color, texture, depth, motion, shape
		\end{itemize}
		\textcolor{red}{Take home message}:\\
		\textbf{\textcolor{red}{For people vision is their most crucial sense, for good reason}}
		\subsection{Perception of vision}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				{\centering Perception of \textbf{intensity}\\}
				\includegraphics[width=\columnwidth, height=2cm]{images/Introduction/perc_intensity.png} 
				The gray fields have the same intensity (same gray tone).
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushright}
				{\centering Perception of \textbf{color}\\}
				\includegraphics[width=\columnwidth, height=2cm]{images/Introduction/perc_color.png}
				The red squares have equal color
			\end{flushright}
		\end{minipage}
		\hrule
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				{\centering Perception of \textbf{length}\\}
				\includegraphics[width=\columnwidth, height=2cm]{images/Introduction/perc_length.png} 
				the horizontal lines are equally long.
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushright}
				{\centering \textbf{Lines being straight}
				\includegraphics[width=2cm, height=2cm]{images/Introduction/perc_straight.png}\\}
				The lines do not have any curvature
			\end{flushright}
		\end{minipage}	
		\hrule
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				{\centering Perception of \textbf{parallelism}\\}
				\includegraphics[width=\columnwidth, height=2cm]{images/Introduction/perc_parallel.png} 
				All lines are parallel.
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushright}
				{\centering Perception of \textbf{curvatures}
				\includegraphics[width=2cm, height=2cm]{images/Introduction/perc_curvature.png}\\}
				There is no spiral.
			\end{flushright}
		\end{minipage}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				{\centering Perception of \textbf{motion}\\}
				\includegraphics[width=\columnwidth, height=3cm]{images/Introduction/perc_motion.png} 
				The pole rotates about the vertical, it does not translate vertically.
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[t]{0.49\columnwidth}
		\begin{flushright}
				{\centering The role of \textbf{context}
				\includegraphics[width=3cm, height=3cm]{images/Introduction/perc_context.png}\\}
				All encircled patterns are identical!
			\end{flushright}
		\end{minipage}
		\par 
		\textcolor{red}{Take home message}:\\
		\textbf{\textcolor{red}{Effective vision needs more than sheer filtering and measuring}}
		\subsection{Applications}
		Most early applications where found in \textbf{production environments}, as these allow for \textbf{controlled conditions} and have \textbf{little uncertainty}. But some areas do not allow for much control: medical IP, remote sensing, surveillance, etc.\\
		Currently Computer Vision (CV) is conquering the less controllable areas by storm:
		\par  
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
		 		{\centering \textbf{Image enhancement:} mobile $\rightarrow$ DSLR\\}
		 		\includegraphics[width=\columnwidth, height=5cm]{images/Introduction/app_enhancement.png} 
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushright}
				{\centering \textbf{Image retrieval, captioning}\\}
				\includegraphics[width=\columnwidth, height=5cm]{images/Introduction/app_retrieval.png}
			\end{flushright}
		\end{minipage}
		\hrule
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				{\centering \textbf{Autonomous vehicles}\\}
				\includegraphics[width=\columnwidth, height=3cm]{images/Introduction/app_autonomous_vehicles.png} 
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushright}
				{\centering \textbf{Visual surveillance}\\}
				\includegraphics[width=\columnwidth, height=3cm]{images/Introduction/app_surveillance.png}
			\end{flushright}
		\end{minipage}
		\hrule
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				{\centering \textbf{Augmented Reality, e.g sports}\\}
				\includegraphics[width=\columnwidth, height=2.5cm]{images/Introduction/app_augm_reality.png} 
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushright}
				{\centering \textbf{Computer-assisted surgery}\\}
				\includegraphics[width=\columnwidth, height=2.5cm]{images/Introduction/app_surgery.png}
			\end{flushright}
		\end{minipage}			
		\par 
		\textcolor{red}{Take home message}:\\
		\textbf{\textcolor{red}{It is feasible now to let most things see and interpret their environment.}}
		
		\subsection{The nature of light}
		There are three different models to describe optical systems: 
		\begin{enumerate}[noitemsep]
			\item Geometrical optics
			\item Physical optics $\rightarrow$ \textcolor{red}{\textbf{wave character}}   
			\item Quantum-mechanical optics
		\end{enumerate}
		In this course we mainly look at physical optics and the wave character of light. 		
		\subsubsection{Light as EM-waves}
		Self-sustaining exchange of electric and magnetic fields. An EM-wave is characterized with the following properties: 
		\includegraphics[width=\columnwidth]{images/Introduction/em_wave.png}			
			\begin{itemize}[noitemsep]
				\item \textbf{Wavelength}
				\item Direction of \textbf{propagation}
				\item \textbf{Amplitude} of E
				\item \textbf{Phase} 
				\item Direction of \textbf{polarization }
			\end{itemize}
		\textbf{The spectrum:}\\
		Normal ambient light is a mixture of wavelengths, polarization directions and phases. The visible range for humans is only a small fraction of the EM-waves-spectrum.\\
		\begin{center}
			\begin{tabular}{c l l}
				\hline 
				\hline
				Wavelength [$nm$] &  & Color \\ 
				\hline 
				380 - 450 & $\rightarrow$ & \textcolor{violet}{violet} \\ 
				450 - 490 & $\rightarrow$ & \textcolor{blue}{blue} \\ 
				490 - 560 & $\rightarrow$ & \textcolor{green}{green} \\ 
				560 - 590 & $\rightarrow$ & \textcolor{yellow}{yellow} \\ 
				590 - 630 & $\rightarrow$ & \textcolor{orange}{orange} \\ 
				630 - 760 & $\rightarrow$ & \textcolor{red}{red} \\
				\hline
				\hline 
			\end{tabular}
		\end{center}
		%\includegraphics[height=3cm]{images/Introduction/visible_spectrum.png}
		The visible range differs from humans to animals and also cameras may have different spectral sensitivities. There are also cameras for non-visible light such as infrared. The following picture shows the three color cones humans have and their sensitivity range: 
		\includegraphics[width=\columnwidth]{images/Introduction/human_cones.png}
		  
		\subsubsection{Interactions with matter}
		We look at the following types of interaction with matter: 
		\begin{enumerate}[noitemsep]
			\item \textbf{Absorption}
				\begin{itemize}[label={$\rightarrow$}]
					\item blue water
				\end{itemize}
			\item \textbf{Scattering}
				\begin{itemize}[label={$\rightarrow$}]
					\item blue sky
					\item red sunset
				\end{itemize}
			\item \textbf{Reflection}
				\begin{itemize}[label={$\rightarrow$}]
					\item colored ink
				\end{itemize} 
			\item \textbf{Refraction}
				\begin{itemize}[label={$\rightarrow$}]
					\item dispersion by a prism
				\end{itemize} 
			\item \textbf{Diffraction} 
		\end{enumerate}
		 We look at few of those in more detail:
		 \par
		  
		 \textbf{1. Absorption}\\
		 A nice example of absorption is earth's atmosphere which absorbs certain wavelengths of the incoming light. The absorbed frequencies correspond to resonance frequencies of molecules in earth's atmosphere.\\ 
		 \includegraphics[width=\columnwidth]{images/Introduction/absorption_earth.png}
		 \clearpage 
		 
		 \textbf{2. Scattering}\\
		 There are three types of scattering depending on the relative sizes of particles and wavelengths:
		 \begin{enumerate}[label=(\alph*)]
		 	\item Small particles: \textcolor{red}{\textbf{Rayleigh}} (strong wavelength dependent)
		 	\item Comparable size: \textcolor{red}{\textbf{Mie}} (weakly wavelength dependent)
		 	\item Large particles: \textcolor{red}{\textbf{Non-selective}} (wavelength independent)
		 \end{enumerate}
	 	If we look at the scattered energy it looks as follows:\\ 
	 	\includegraphics[width=\columnwidth]{images/Introduction/scattered_energy.png}
	 	
	 	Let's see some examples of these different scatter-types in our atmosphere:\\ 
	 	\begin{minipage}[t]{0.49\columnwidth}
	 		\begin{flushleft}
	 			\includegraphics[width=\columnwidth]{images/Introduction/ex_ray_non.png}\\
	 		\end{flushleft}
	 	\end{minipage}
	 	\begin{minipage}[b]{0.49\columnwidth}
	 		\begin{flushleft}
	 			\textbf{Rayleigh}: Tyndall effect (blue sky, red setting sun)\\
	 			\textbf{Non-selective}: Grey clouds
	 		\end{flushleft}
	 	\end{minipage}
 		%\hrule
 		\par 
 		\begin{minipage}[t]{0.49\columnwidth}
 			\begin{flushleft}
 				\includegraphics[width=\columnwidth]{images/Introduction/ex_mie.png}\\
 			\end{flushleft}
 		\end{minipage}
 		\begin{minipage}[b]{0.49\columnwidth}
 			\begin{flushleft}
 				\textbf{Mie}: Colored cloud from volcanic eruption
 				\vspace{0.6cm}
 			\end{flushleft}
 		\end{minipage}
		\par 
		\textbf{3. Reflection:}\\
		In mirror reflection we have:\\ angle of reflection = angle of incident.\\
		Two different categories of reflective materials:\\
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth]{images/Introduction/refl_diel.png}\\
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[b]{0.49\columnwidth}
			\begin{flushleft}
				\textbf{Dielectric:}\\
				For parallel polarization there exists the Brewster angle where $r=0$.
				\vspace{1.2cm}
			\end{flushleft}
		\end{minipage}
		\par 
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth]{images/Introduction/refl_metal.png}\\
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[b]{0.49\columnwidth}
			\begin{flushleft}
				\textbf{Conductor:}\\
				Strong reflectors under all angles, more or less preserve polarization.
				\vspace{1.2cm}
			\end{flushleft}
		\end{minipage}
	
		We differentiate three types of reflection which depend on the surface structure: 
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth]{images/Introduction/refl_diffuse.png}\\
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[b]{0.49\columnwidth}
			\begin{flushleft}
				\textbf{Diffuse:}\\
				Also called \textbf{Lambertian}. Rough surfaces.
				\vspace{0.2cm}
			\end{flushleft}
		\end{minipage}
		\par
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth]{images/Introduction/refl_mirror.png}\\
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[b]{0.49\columnwidth}
			\begin{flushleft}
				\textbf{Specular:}\\
				Mirror-like surfaces.
				\vspace{0.5cm}
			\end{flushleft}
		\end{minipage}
		\par 
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth]{images/Introduction/refl_mixed.png}\\
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[b]{0.49\columnwidth}
			\begin{flushleft}
				\textbf{Mixed:}\\
				Mix of diffuse and specular.
				\vspace{0.5cm}
			\end{flushleft}
		\end{minipage}
		\par 
		\textbf{4. Refraction:}\\
		\begin{minipage}[t]{0.39\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth]{images/Introduction/snells_law.png}\\
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[b]{0.59\columnwidth}
			\begin{flushleft}
				Effect of the bending of light if it hits an interface of two materials with different refraction index $n=\sqrt{\epsilon\mu}$. The bending is described through \textbf{\textcolor{red}{Snell's law}}:\\
				%\vspace{0.2cm}
				\ceqbox{n_1\sin\theta_1=n_2\sin\theta_2}
				\vspace{1.1cm}
			\end{flushleft}
		\end{minipage}
		\par 
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth]{images/Introduction/dispersion.png}\\
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[b]{0.49\columnwidth}
			\begin{flushleft}
				\textbf{Dispersion:}\\
				The bending is dependent of the frequency (wavelength) of the light. 
				\vspace{0.2cm}
			\end{flushleft}
		\end{minipage}
		\vfill\null
		\columnbreak

		\section{Image Acquisition}	
		\subsection{Illumination}
		Well designed illumination often is key in visual inspection and can extremely simplify the image processing. Here is an overview of different illumination techniques:
		
		\subsubsection{Back-lighting}
		\begin{center}
			\includegraphics[width=0.7\columnwidth,]{images/ImageAcq/back_lighting.png}\\
		\end{center}
		
		\textbf{How:}\\
		Lamps are placed behind a transmitting diffuser plate which is located behind the object
		\par 
		\textbf{Why:}\\
		Creates \textbf{high-contrast} silhouette images, easy to handle with \textbf{binary vision} (only two intensity levels, black and white). Often used in inspection.
	
		\subsubsection{Directional-lighting}
		\begin{center}
			\includegraphics[width=0.7\columnwidth,]{images/ImageAcq/directional_lighting.png}\\
		\end{center}
	
		\textbf{How:}\\
		Light source shines directly on object, maybe under a certain angle.
		\par 
		\textbf{Why:}\\
		\vspace{-0.5cm}
		\begin{itemize}[noitemsep]
			\item Generation of \textbf{specular reflection} (e.g. crack detection (see figure above))
			\item Generation of \textbf{sharp shadows}
		\end{itemize}
		\vfill\null
		\columnbreak
		
		\subsubsection{Diffuse-lighting}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth]{images//ImageAcq/diffusive_lighting.png}\\
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[b]{0.49\columnwidth}
			\begin{flushleft}
				\textbf{Left:}\\
				Direct lighting produced large changes in brightness due to specular reflection.\\
				\vspace{0.2cm}
				\textbf{right:}\\
				Diffusive lighting reduces bright spots. 
				\vspace{0.2cm}
			\end{flushleft}
		\end{minipage}
	
		\textbf{How:}\\
		Do not directly shine with light source on object, but rather indirectly with the help of a diffusive surface. It does not reduces the specular reflection, but increases the diffuse reflection component, yielding in less variations.
		\par 
		\textbf{Why:}\\
		Prevents sharp shadows and large intensity variations over glossy surface.  
	
		\subsubsection{Polarized-lighting}
		The polarization direction is the one of the E-Wave. Normally, light is composed of many waves with different polarizations. \\
		Following picture shows a polarizer/analyzer configuration:\\
		\includegraphics[width=\columnwidth]{images//ImageAcq/polarizer_analyzer.png}
		The intensity seen from the observer depends on the angle $\theta$ and is described by the \textcolor{red}{\textbf{law of Malu}s}:\\
		\ceqbox{I(\theta)=I(0)\cos^2\theta}
		There are 2 uses for polarized lighting:
		\begin{enumerate}[noitemsep]
			\item Improve contrast between Lambertian and specular reflection.
			\item Improve contrast between dielectrics and metals.
		\end{enumerate}
		\vfill\null
		\columnbreak
		
		\textbf{1. Specular vs. Lambertian}\\
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images//ImageAcq/diffuse_specular.png}\\
		\end{center}
		\textbf{How:}\\
		Polarizer and analyzer in crossed arrangement. Specular reflection keeps polarization, Lambertian reflection depolarizes, because of this the arrangement reduces the large dynamic range caused by glare. 
		\par 
		\textbf{Why:}\\
		Increases contrast between Lambertian and specular reflection (specular reflection gets blocked).
		
		\textbf{2. Dielectric vs. Metal}\\
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images//ImageAcq/dielectric_metal.png}\\
		\end{center}
		\textbf{How:}\\
		Shine non-polarized light at Brewster angle on object. The dielectric will not reflect the parallel polarized component (Brewster angle...) and the perpendicular component is filtered out by the analyzer, hence the dielectric parts of the object will be really dark.  
		\par 
		\textbf{Why:}\\
		Increases contrast between dielectrics and metals (dielectric reflection gets blocked).
		\vfill\null
		\columnbreak
		
		\subsubsection{Colored-lighting}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth]{images//ImageAcq/white_light.png}\\
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[b]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth]{images//ImageAcq/red_light.png}\\
			\end{flushleft}
		\end{minipage}
		The contrast between the red label and the green PCB is increased when red light is shined on it (right) in comparison to white light (left).\\
		\par 
		\textbf{How:}\\
		Shine colored light at an object and maybe also use an bandpass filter. You need to have in mind the spectral sensitivity of your sensor!
		\par 
		\textbf{Why:}\\
		\vspace{-0.5cm}
		\begin{itemize}[noitemsep]
			\item Highlight regions of similar color
			\item Differentiate between specular and diffuse reflection
			\item Comparing color
		\end{itemize}
		
		\subsubsection{Structures-lighting}
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images//ImageAcq/structured_lighting.png}\\
		\end{center}
		\textbf{How:}\\
		Spatially or temporally modulated light patterns are shined on a 3D object.
		\par 
		\textbf{Why:}\\
		Obtain 3D info of object.
		\par
		More on this later...
	
		\subsubsection{Stroboscopic-lighting}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth, height=2cm]{images//ImageAcq/blur.png}\\
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth, height=2cm]{images//ImageAcq/no_blur.png}\\
			\end{flushleft}
		\end{minipage}
		\par 
		\textbf{How:}\\
		High intensity light flashes. The light flash artificially shortens the sensors integration time. Mostly this is much cheaper than fast cameras.
		\par 
		\textbf{Why:}\\
		Eliminate motion blur
		
		\subsection{Cameras}
		\subsubsection{Camera models}
		\textbf{Pinhole-model}\\
		\begin{minipage}[b]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth, height=2cm]{images//ImageAcq/pinhole_1.png}\\
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[t]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth, height=2cm]{images//ImageAcq/pinhole_2.png}\\
			\end{flushleft}
		\end{minipage}
		Light enters the box only through a small hole and an image is formed on a plane inside the box. If the whole is too small not enough light enters and additional diffraction occurs, if the whole is too big, the image gets blurry. The solution to this problem is a lens. From similar triangles the following formula follows:
		\begin{center}
			$\dfrac{X_i}{X_o}=\dfrac{Y_i}{Y_o}=\dfrac{f}{Z_o}=-m=$ \textit{linear magnification}\\
		\end{center}
		\vspace{0.5cm}
		\textbf{The thin-lens model}\\
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images//ImageAcq/thin_lens.png}\\
		\end{center}
		A lens captures more light and focuses it which gets rid off the problems of the pinhole. The price we pay is that only points at certain plane will be sharply imaged. Similar to the pinhole model the \textcolor{red}{\textbf{thin lens equation}} reads as:
		\ceqbox{\dfrac{1}{Z_o}-\dfrac{1}{Z_i}=\dfrac{1}{f}}
		\par 
		\textbf{The depth-of-field}\\
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images//ImageAcq/depth_of_field.png}\\
		\end{center}
		As already mentioned only at a specific plane the image will be sharp, we can define an interval $\Delta Z_o^-$ in which we have a reasonable sharpness of the image:
		\begin{center}
			$\Delta Z_o^-=Z_o-Z_o^-=\dfrac{Z_o(Z_o-f)}{Z_o+fd/b-f}$\\
		\end{center}
		The depth-of-field \textbf{decreases with $\mathbf{d}$} and \textbf{increases with $\mathbf{Z_o}$}. The more we focus with the lens, the smaller the depth-of-field is, because de rays diverge stronger outside of the focal point. There is also a trade off between collecting a lot light (big $d$) and a large depth-of-field (usable depth range).
		\par 
		\textcolor{red}{Take home message}:\\
		\textbf{\textcolor{red}{In summary, introducing a lens to some extent solves the problem of insufficient light reaching the light detecting area of the camera. The price we pay is the loss of overall sharpness, i.e. points at a distance outside some range are no longer in focus.}}
		
		\subsubsection{Aberrations}
		In the above lens-model we made three assumptions:
		\begin{enumerate}[noitemsep]
			\item All rays from a point are focused onto 1 image point
			\item All image points in a single plane
			\item Magnification is constant
		\end{enumerate}
		Deviations form this ideal are called \textcolor{red}{\textbf{aberrations}}. We differentiate between two types of aberrations:
		\begin{enumerate}[noitemsep]
			\item \textbf{Geometrical}: visible as image distortions or degradation like blurring
				\begin{itemize}
					\item Spherical aberration
					\item astigmatism
					\item \textbf{radial distortion} (most important)
					\item coma
				\end{itemize}
			\item \textbf{Chromatic}: visible as different behavior for different wavelengths (color)
		\end{enumerate}
		
		\textbf{Spherical Aberration:}\\
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images/ImageAcq/spherical_aberration.png}\\
		\end{center}
		Rays parallel to the optical axis do not converge, because outer parts of the length yield smaller focal lengths. This results in blurry edges on the image.
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images/ImageAcq/spherical_aberration_2.png}\\
		\end{center}
		\vfill\null
		\columnbreak
		
		\textbf{Radial distortion:}\\
		\includegraphics[width=\columnwidth]{images//ImageAcq/radial_dist.png}\\
		Different magnification for different angles of incident. This results in: 
		\begin{itemize}
			\item Lines become curves
				\begin{itemize}[label={$\rightarrow$}]
					\item Curvature increases as you move away form the center of distortion
						\begin{itemize}[label={$\rightarrow$}]
							\item Model assume this is the image center and there is a multiplicative factor on the pixel depending on the distance $r$ to the center:
							\vspace{-0.4cm}
							\ceqbox{d=1+\kappa_1r^2+\kappa_2r^4+\dots}
							Only even factors because effect is symmetric.\\
							This aberration can be corrected by software if the parameters $\kappa_1, \kappa_2, \dots$ are known.
						\end{itemize} 
				\end{itemize}
		\end{itemize}
		\includegraphics[width=\columnwidth]{images/ImageAcq/radial_dist_2.png}\\
		\par 
		\textbf{Chromatic distortion:}
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images//ImageAcq/chromatic_aberration.png}\\
		\end{center}
		Rays fo different wavelength focused on different planes. This can not be removed completely, but \textcolor{red}{\textbf{achromatization}} can be achieved at some well chosen wavelength pair, by combining lenses made of different glasses: 
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images//ImageAcq/achromatization.png}\\
		\end{center}
		Sometimes achromatization is achieved for more than two wavelengths. 
		
		\subsubsection{Device technologies}
		We consider 2 types: 
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images//ImageAcq/ccd_cmos.png}\\
		\end{center}
		\begin{tabular}{l l}
			\hline 
			\hline
			\thead{CCD} &  \thead{CMOS}  \\ 
			\hline
			Niche applications    & Consumer cameras  \\ 
			Specific technology   & Standard IC technology\\ 
			Expensive product.	  & Cheap \\ 
			High power 			  & Low power \\ 
			Higher fill rate 	  & Less sensitive  \\ 
			Blooming			  & Per pixel amplif. \\ 
			Sequential readout	  & Random pixel access\\ 
								  & Smart pixels \\
								  & On chip with other comp.\\
			\hline 
			\hline
		\end{tabular}
		\par
		In 2006 was year of sales cross-over and in 2015 Sony said to stop CCD chip production. 
		\subsubsection{Color cameras}
		We consider 3 concepts: 
		\begin{enumerate}[noitemsep]
			\item Prism (with 3 Sensors)
			\item Filter mosaic 
			\item Filter wheel 
		\end{enumerate}
		\vfill\null
		\columnbreak
		
		\textbf{1. Prism color camera}\\
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images/ImageAcq/prism_color_camera.png}\\
		\end{center}
		Separates light in 3 different beams using dichroic prism\\ 
		\textcolor{red}{ -- Requires 3 Sensors \& precise alignment}\\
		\textcolor{ForestGreen}{+ Good color separation}
		\par 
		\textbf{2. Filter mosaic}\\
		\vspace{-0.3cm}
		\begin{center}
			\includegraphics[width=0.9\columnwidth]{images/ImageAcq/bayer_filter.png}\\
		\end{center}
		Filter is directly coated on sensor. \textbf{Microlenses} are used to gain more light on pixels, because effective resolution is reduced by the filter.\\
		\textcolor{red}{ -- Reduces resolution}\\
		\textcolor{ForestGreen}{+ Chap and easy}
		\par 
		\textbf{3. Filter wheel}\\
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images/ImageAcq/filter_wheel.png}\\
		\end{center}
		Rotate multiple filters in front of lens
		\textcolor{red}{ -- Only suitable for static scenes}\\
		\textcolor{ForestGreen}{+ Allows more than 3 color bands}
		
		\begin{tabular}{l l l l}
			\hline 
			\hline 
			Approach  &Prism  &Mosaic  &Wheel  \\
			\hline  
			\#Sensors &3  	 &1  	  &1  \\ 
			Resolution&High  &Average &Good  \\ 
			Cost	  &High  &Low	  &Average  \\ 
			Framerate &High  &High	  &Low  \\ 
			Artefacts &Low	 &Aliasing&Motion  \\ 
			Bands	  &3  	 &3  	  &3 or more  \\
					  &High-end &Low-end  &Scientific  \\  
			\hline 
			\hline 
		\end{tabular} 
	
		\subsubsection{Geometric models}
		\textbf{Perspective projection}\\
		Perspective projection is the projection of the object to a \textbf{virtual image plane} in front of the lens. We choose the virtual image plane so that we do not need to care about the rotation of the picture. The \textbf{center of projection} is the \textbf{center of the lens} (pinhole).\\
		\includegraphics[width=\columnwidth]{images/ImageAcq/perspective_projection.png}\\
		\begin{itemize}[noitemsep]
			\item \textit{Camera coordinate frame} (hence subscript $c$) lies at center of projection.
			\item $Z_c$ coincides with the optical axis of the lens/objective.
			\item $X_c$ is parallel to image rows, $Y_c$ is parallel to image columns.
			\item $u$ and $v$ axes are parallel to $X_c$ and $Y_c$ axes. 
			\item \textit{Principal point} = point where optical axis intercepts image plane.
			\item The image of a point \textbf{P} $(X_c,Y_c,Z_c)$ is the intersection of the line through \textbf{P} and the center of projection with the image plane (plane at $f$).
		\end{itemize}
		The $(u,v)$-coordinates of this image point are directly found through similar triangles:\\
		\ceqbox{u=f\dfrac{X_c}{Z_c}\qquad v=f\dfrac{Y_c}{Z_c}}
		This model is an approximation, because for the image plane to lie a focal distance from the center of projection assumes the object to lie at a large distance compared to the focal length.
		\par 
		\textbf{Pseudo-orthographic projection}\\
		First, if $Z$ is constant for all points of an object:
		\ceqbox{x=kX\qquad y=kY,\qquad k=f/Z}
		Second, this is also a good approximation if $f/Z\approx constant$, i.e objects are small compared to distance from camera.
		\par 
		\textbf{Projection matrices}\\
		The perspective projection model is not very practical and incomplete in at least 2 ways: 
		\begin{enumerate}[noitemsep]
			\item 3D coordinates are specified in a \textit{world coordinate frame}, not the camera coordinate frame.
			\item Image coordinates ($u,v$) must be translated to row and column numbers (pixels). 
		\end{enumerate}
		No additional refinements such as radial distortions are considered. 
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images/ImageAcq/rotation_matrices.png}\\
		\end{center}
		The position of the Camera is described by a point \textbf{C} (center of projection) and a $3\times3$ rotation matrix $\R$ where $\mathbf{r}_i$ denotes the $i$th column of the matrix. $\mathbf{r}_1$ corresponds to $X_c$, $\mathbf{r}_2$ to $Y_c$ and $\mathbf{r}_3$ to $Z_c$ of the camera coordinate frame. The corresponding image point then has $(u,v)-$coordinates:
		\ceqbox{u=f\dfrac{\langle\mathbf{r}_1,\mathbf{P}-\mathbf{C}\rangle}{\langle\mathbf{r}_3,\mathbf{P}-\mathbf{C}\rangle}\quad \mathrm{and} \quad
				v=f\dfrac{\langle\mathbf{r}_2,\mathbf{P}-\mathbf{C}\rangle}{\langle\mathbf{r}_3,\mathbf{P}-\mathbf{C}\rangle}} 
		If \textbf{P} has coordinates $(X,Y,Z)$ and \textbf{C} has coordinates $(C_1,C_2,C_3)$ with respect to the \textit{world frame}, then:
		\par   
		$u=\dfrac{r_{11}(X_1-C_1)+r_{12}(Y-C_2)+r_{13}(Z-C_3)}{r_{31}(X_1-C_1)+r_{32}(Y-C_2)+r_{33}(Z-C_3)}$
		\par
		$v=\dfrac{r_{21}(X_1-C_1)+r_{22}(Y-C_2)+r_{23}(Z-C_3)}{r_{31}(X_1-C_1)+r_{32}(Y-C_2)+r_{33}(Z-C_3)}$
		\par 
		When working with digital images we want to define the position of an image point in so-called \textit{pixel coordinates}. A digital image thus is a 2-dimensional array of pixels, say $m$-columns and $n$-rows:\\
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images/ImageAcq/pixel_coordinates.png}\\
		\end{center}
		We want to indicate the position of an image point by its column and row number instead of the $(u,v)$-coordinates.
		\begin{align*}
			x&=k_xu+sv+x_0\\
			y&=k_yv+y_0
		\end{align*}
		Here:
		\vspace{-0.2cm}
		\begin{itemize}[noitemsep]
			\item $(x_0,y_0)$ are the pixel coordinates of the principal point.
			\item $k_x$ is the number of pixels per unit length in horizontal direction (describes 1/width of a pixel)
			\item $k_y$ is the number of pixels per unit length in vertical direction (describes 1/height of a pixel)
			\item $k_x/k_y$ \textit{aspect ratio}, if $\neq1$, then pixel ware not square
			\item $s$ indicates the skew, how much pixel deviates from a rectangular. 
		\end{itemize}
		$k_x, k_y, s, x_0, y_0$ are called \textcolor{red}{\textbf{internal camera parameters}}. When they are known the camera is \textcolor{red}{\textbf{internally calibrated}}.
		\par 
		Vector $\mathbf{C}$ and matrix $R$ are the \textcolor{red}{\textbf{external camera parameters}}. When they are known the camera is \textcolor{red}{\textbf{externally calibrated}}. 
		\par 
		\textcolor{red}{\textbf{Fully calibrated}} means internally and externally calibrated.
		\par 
		Perspective projection being a projective mapping, the image formation process in a pinhole camera (as described above) can be represented \textbf{linearly} by using \textbf{homogeneous coordinates}. Examples:\\
		2D:\hspace{0.2cm}$\begin{pmatrix}x\\y\\z\end{pmatrix}\rightarrow\begin{pmatrix}x/z\\y/z\end{pmatrix}$\hspace{0.5cm}
		3D:\hspace{0.2cm}$\begin{pmatrix}X\\Y\\Z\\W\end{pmatrix}\rightarrow\begin{pmatrix}X/W\\Y/W\\Z/W\end{pmatrix}$\\
		Exploiting homogeneous coordinates we can write $(u,v)$ as:
		\par
		$\tau\begin{pmatrix}u\\v\\1\end{pmatrix}=
		\begin{pmatrix}fr_{11}&fr_{12}&fr_{13}\\fr_{21}&fr_{22}&r_{23}\\r_{31}&r_{32}&r_{33}\end{pmatrix}
		\begin{pmatrix}X-C_1\\Y-C_2\\Z-C_3\end{pmatrix}$
		\par
		and $(x,y)$ as:
		\par 
		$\tau\begin{pmatrix}x\\y\\1\end{pmatrix}=
		\begin{pmatrix}k_x&s&x_0\\0&k_y&y_0\\0&0&1\end{pmatrix}
		\tau\begin{pmatrix}u\\v\\1\end{pmatrix}$
		\par
		Concatenating the results:
		\begin{align*}
			\tau\begin{pmatrix}x\\y\\1\end{pmatrix}=&\begin{pmatrix}k_x&s&x_0\\0&k_y&y_0\\0&0&1\end{pmatrix}
			\begin{pmatrix}f&0&0\\0&f&0\\0&0&f\end{pmatrix}\cdot\\
			&\begin{pmatrix}r_{11}&r_{12}&r_{13}\\r_{21}&r_{22}&r_{23}\\r_{31}&r_{32}&r_{33}\end{pmatrix}
			\begin{pmatrix}X-C_1\\Y-C_2\\Z-C_3\end{pmatrix}
		\end{align*}
		This yields the \textbf{calibration matrix} $K$:
		\begin{align*}
			K&=\begin{pmatrix}k_x&s&x_0\\0&k_y&y_0\\0&0&1\end{pmatrix}\begin{pmatrix}f&0&0\\0&f&0\\0&0&f\end{pmatrix}\\
			 &=\begin{pmatrix}fk_x&fs&x_0\\0&fk_y&y_0\\0&0&1\end{pmatrix}
		\end{align*}
		We define: $\mathbf{p}=\begin{pmatrix}x\\y\\1\end{pmatrix};\ \mathbf{P}=\begin{pmatrix}X\\Y\\Z\end{pmatrix};\ 
					\mathbf{\tilde{P}}=\begin{pmatrix}X\\Y\\Z\\1\end{pmatrix}$
		\par 
		yielding the \textcolor{red}{\textbf{projection equation}} for a pinhole camera, for some non-zero $\rho\in\mathbb{R}$: 
		\ceqbox{\rho\mathbf{p}=KR^t\left(\mathbf{P}-\mathbf{C}\right)}
		If also homogeneous coordinates are used for the scene points, then the projection equation becomes: 
		\par 
		\hspace{0.3cm}$\rho\mathbf{p}=K\left(R^t\ \vert\ -R^t\mathbf{C}\right)\mathbf{\tilde{P}}\qquad\vert:=$concatenate
		\par 
		or,
		\par 
		\hspace{0.3cm}$\rho\mathbf{p}=K\left(M\ \vert\ \mathbf{t}\right)\mathbf{\tilde{P}}\qquad$ with $\text{rank} \ M = 3$
		\par 
		The last equation follows from linear algebra from which we see that $KR^t$ can be any invertible $3\times3$ matrix $M$
		and that $-KR^t\mathbf{C}$ can be any column vector $\mathbf{t}\in\mathbb{R}^3$.
		\par 
		
		\subsubsection{Photometric camera model}
		From object radiance to pixel gray level in 2 steps: 
		\begin{enumerate}[noitemsep]
			\item From object radiance to image irradiance
			\item From image irradiance to pixel gray level. 
		\end{enumerate}
		\textbf{1. From object radiance to image irradiance:}\\
		We look at the irradiance that an object patch will cause in the image. Assumptions:
		\begin{itemize}[noitemsep]
			\item Radiance $R$ is known
			\item Object at far distance compared to focal length
		\end{itemize}
		Is image irradiance directly related to radiance of the object patch? We look at the following viewing condition: 
		\begin{center}
			\includegraphics[width=0.7\columnwidth]{images/ImageAcq/image_irradiance.png}\\
		\end{center}
		With a few calculations and a little black magic we obtain the \textcolor{red}{$\mathbf{\cos^4}$ \textbf{law}}:
		\ceqbox{I=R\dfrac{A_l}{f^2}\cos^4\alpha}
		\begin{minipage}[b]{0.49\columnwidth}
			\begin{flushleft}
				\includegraphics[width=\columnwidth, height=2cm]{images/ImageAcq/fish_eye.png}\\
			\end{flushleft}
		\end{minipage}
		\begin{minipage}[b]{0.49\columnwidth}
			\begin{flushleft}
				This effect is especially strong for wide-angle and fish eye lenses. 
				\vspace{0.5cm}
			\end{flushleft}
		\end{minipage}
	
		\textbf{2. From irradiance to gray levels}\\
		The relation of irradiance to gray level has the form of: 
		\ceqbox{f=gI^{\gamma}+d}
		\begin{tabular}{l l}
			f:&  resulting gray level\\ 
			I:&  irradiance\\ 
			g:&  so-called \textit{gain} of camera (constant)\\ 
			$\gamma$:&  non-linearity of the camera \\ 
			d:&  \textit{dark reference}\\  
		\end{tabular}
		\par 
		The \textit{gain} $g$ can be set with the diaphragm size. Nowadays $\gamma$ is pretty close to 1. The \textit{dark reference} $d$ is the signal measured with the lens cap on. 
		\par 
		
		
		\section{Feature Extraction}	
		
		 
		
	\end{multicols*}
	\setcounter{secnumdepth}{3}
\end{document}
