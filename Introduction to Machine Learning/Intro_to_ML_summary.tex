%
% Zusammenfassung Introduction to Machine Learning
% ===========================================================================
% Author:			Marco Dober
% Version:			0.1
% Last changed: 	01.03.2020	
% ---------------------------------------------------------------------------

\documentclass[a4paper, fontsize=8pt, landscape, DIV=1]{scrartcl}
\usepackage{lastpage}
\usepackage{hyperref, makecell}
\usepackage[graphicx]{realboxes}
% Include general settings and customized commands
\input{settings/general}
\input{settings/commands}
%change page style for header
\pagestyle{fancy}
\footskip 20pt
\rhead{Marco Dober}
\lhead{Introduction to Machine Learning}
\chead{\thepage}
\cfoot{}
\headheight 17pt \headsep 10pt
\title{Introduction to Machine Learning}
\author{Marco Dober}
\date{\today}


\begin{document}
	\setcounter{secnumdepth}{3} %no enumeration of sections
	\begin{multicols*}{4}
		%
		\section*{Disclaimer}
		This summary is part of the lecture ``Introduction to Machine Learning'' by Prof. Krause (FS20). It is based on the lecture slides. \\[6pt]
		Please report errors to \href{mailto:doberm@student.ethz.ch}{doberm@student.ethz.ch} such that others can benefit as well.\\[6pt]	
		The upstream repository can be found at \href{https://github.com/mrrebod/Summaries}{https://github.com/mrrebod/Summaries}
		\vfill\null
		\pagebreak
		
		\maketitle 
		\thispagestyle{fancy}
		
		\section{Introduction}
		There exists two basic forms of learning:
		\begin{itemize}[noitemsep]
			\item \textbf{Supervised learning}\\
			You have data with x- and y-labels $\rightarrow$ try to fit model $\rightarrow$ predict with found model new cases. Examples of supervised learning:
			\begin{itemize}
				\item Classification
				\item Regression
				\item Structured prediction, ...
			\end{itemize}
			Model selection is very important and a trade off between complexity vs. goodness of fit. Ideal models are statistically and computationally efficient. 
			\includegraphics[width=\columnwidth]{images/Introduction/model_selection.png}
			\item \textbf{Unsupervised learning}\\
			"Learning without labels" (no y-label on data). Examples:
			\begin{itemize}[noitemsep]
				\item Clustering (e.g., unsupervised classification)
				\item Dimension reduction (e.g., unsupervised regression)
				\item Generative modeling
			\end{itemize}
			Common goals: 
			\begin{itemize}
				\item Compact representation / compression of data sets
				\item Identification of latent variables
			\end{itemize}
		\end{itemize}
		Example of unsupervised learning, Clustering: Input data with no labels and assign data to clusters
		\includegraphics[width=0.85\columnwidth]{images/Introduction/clustering.png}\\
		Other models of learning: semi-supervised, transfer, active, online, reinforcement, ... .\\
		The key challenge in ML is: \\
		$\bullet$ Trading goodness of fit and model complexity\\
		$\bullet$ Also the \textbf{representation of data} is of key importance. 
		
		\section{Regression}
		Is an instance of supervised learning.\\
		\textbf{Goal:} Predict real valued labels.\\
		Important choices in regression:
		\vspace{-0.1cm}
		\begin{itemize}[noitemsep,nolistsep]
			\item What \textbf{types of functions} should we consider?
			\item How should we measure \textbf{goodness of fit}?
		\end{itemize}
		
		\subsection{Linear Regression}
		$y\approx f(x)$, $f$ is linear (affine) in $x$
		\includegraphics[width=\columnwidth]{images/Regression/linear_regression.png}
		\begin{itemize}[noitemsep]
			\item 1-d: $f(x)=ax+b$, (line)
			\item 2-d: $f(x)=ax_1+bx_2+c$, (area)
			\item d-d: $f(x)=w_1x_1+\dots+w_dx_d+w_0=\\\sum_{i=1}^{d}w_ix_i+w_0=w^Tx+w_0$\\
			$\mathbf{w}=[w_1,\dots,w_d]^T$, $\mathbf{x}=[x_1,\dots,x_d]^T$
		\end{itemize}
		Homogeneous representation (no offset):\\
		$\mathbf{w}^T\mathbf{x}+w_0=\tilde{\mathbf{w}}^T\tilde{\mathbf{x}}$\\
		$\tilde{\textbf{w}}=[w_1,\dots,w_d,w_0]^T$, $\tilde{\textbf{x}}=[x_1,\dots,x_d,1]^T$\\
		
		\subsubsection{Quantifying goodness of fit}
		Calculate \textbf{residuals}, difference from fitet-point and data-point. Different ways to compute loss function (abs-value, square, ...), see below. Most famous is the \textbf{least-square optimization}: \\
		$r_i=y_i-f(\mathbf{x}_i)=y_i-\textbf{w}^T\textbf{x}_i$\\
		Cost $\hat{R}(w)=\sum_{i=1}^{n}r_i^2=\sum_{i=1}^{n}(y_i-w^Tx_i)^2$\\
		How do we find the optimal weight vector $\textbf{w}^*$?
		
		\ceqbox{\textbf{w}^*=\arg\min\limits_w\sum_{i=1}^{n}(y_i-\textbf{w}^T\textbf{x}_i)^2} 

		\columnbreak
		
		$\bullet$ \textbf{Closed form}\\
		The problem from above can be solved in closed form!
		
		\ceqbox{\textbf{w}^*=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}}

		$\textbf{X}=\begin{pmatrix} x_{1,1} & \dots & x_1,d \\ \dots & \dots & \dots \\ x_{n,1} & \dots & x_{n,d} \end{pmatrix}$, $\textbf{y}=\begin{pmatrix} y_1 \\ \dots \\ y_n \end{pmatrix}$\\
		\vspace{0.1cm}
		
		$\bullet$ \textbf{Optimization}\\
		The objective function $\hat{R}(\textbf{w})=\sum_{i=1}^{n}(y_i-\textbf{w}^T\textbf{x}_i)^2$ is \textbf{convex}. In convex functions local minimum = global minimum!
		\vspace{0.1cm}
		
		\textbf{Gradient descent}:\\
		- Start at arbitrary $\textbf{w}_0 \in \R^d$\\
		- For $t=1,2,...$ do
		\begin{equation*}
			\textbf{w}_{t+1}=\textbf{w}_t-\eta_t\nabla\hat{R}(\textbf{w}_t)
		\end{equation*}
		If I descend along the gradient I will for sure find a minimum (step size / learning rate $\eta_t$ sufficiently small). For convex objectives it finds an optimum. For the squared loss, \textbf{constant step size 0.5 converges linearly}!\\
		$\nabla\hat{R}(\textbf{w})=\dots=-2\sum_{i}^{n}r_i\textbf{x}_i^T$, (in all dimensions)
		\vspace{0.1cm}
		
		\textbf{Adaptive step size}\\
		- step size too big: oscillation, divergence\\
		- step size too small: long computation\\
		Different ways to update step size adaptively: 
		\begin{enumerate}[noitemsep]
			\item Line search (optimizing step size every step)\\
			$g_t=\nabla\hat{R}(\textbf{w}_t)$\\
			$\eta_t = \arg \min\limits_{\eta\in(0,\inf)}\hat{R}(\textbf{w}_t-\eta g_t)$
			\item Bold driver heuristic
			\begin{itemize}
				\item If function decreases, increase step size:\\
				$\hat{R}(\textbf{w}_t-\eta_t g_t)<\hat{R}(\textbf{w}_t)\Rightarrow \eta_{t+1}=\eta_t\cdot c_{acc.}$ 
				\item If function increases, decrease step size:\\
				$\hat{R}(\textbf{w}_t-\eta_t g_t)>\hat{R}(\textbf{w}_t)\Rightarrow \eta_{t+1}=\eta_t\cdot c_{dec.}$\\
				$c_{acc.}>1$ \& $c_{dec.}<1$
			\end{itemize}
		\end{enumerate}
		\textbf{Gradient descent vs. closed form} (many times gradient descent (optimization) is better than closed form):
		\vspace{-0.2cm} 
		\begin{itemize}[noitemsep]
			\item computational complexity:\\
			closed form can get very ugly (shit-ton of calculations)
			\item May not need an optimal solution:\\
			Uncertainty in data, so it makes no sense to find the optimal solution, I can stop earlier with optimization.
			\item Many problems don't admit closed form solution
		\end{itemize}
		\columnbreak
		
		\includegraphics[width=\columnwidth]{images/Regression/convex_nonconvex.png}
		\includegraphics[width=\columnwidth]{images/Regression/gradient_descent.png}
		\subsubsection{Other loss functions}
		so far we used squared error to measure goodness of fit, but there are many other possibilities. 
		\includegraphics[width=\columnwidth]{images/Regression/loss_functions.png}
		$l_p(r)=\abs{r}^p=\abs{y_i-\textbf{w}^T\textbf{x}_i}^p$, with the parameter $p$ you can vary your optimization. 
		\begin{itemize}[noitemsep]
			\item $p=1$: weight all $r$ the same, less sensible to outliers 
			\item $p=2$: closed form sol., most common
			\item $p>>1$: make tolerance band of allowed errors
			\item $p<<1$: almost no weight to outliers
		\end{itemize}
		$\hat{R}(\textbf{w})=\sum_{i=1}^{n}l_p(y_i-\textbf{w}^T\textbf{x}_i)$
		\newpage 
		
		\subsection{Fitting nonlinear functions}
		We can fit nonlinear functions via linear regression using nonlinear features of our data (basis functions). With a nonlinear transformation we transform data to be able to fit linearly.\\
		
		
	\end{multicols*}
	\setcounter{secnumdepth}{3}
\end{document}
